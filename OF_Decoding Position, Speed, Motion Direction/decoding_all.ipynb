{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Murano/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#for mat file \n",
    "import scipy.io as sio\n",
    "import seaborn as sb\n",
    "\n",
    "# import user defined functions\n",
    "from functions import *\n",
    "from utils import * \n",
    "\n",
    "# Import decoder functions\n",
    "from decoders import WienerCascadeDecoder\n",
    "from decoders import WienerFilterDecoder\n",
    "from decoders import DenseNNDecoder\n",
    "from decoders import SimpleRNNDecoder\n",
    "from decoders import GRUDecoder\n",
    "from decoders import LSTMDecoder\n",
    "from decoders import XGBoostDecoder\n",
    "from decoders import SVRDecoder\n",
    "from decoders import KalmanFilterDecoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mat_file(res_all,result_file):\n",
    "\n",
    "    #save decoding result in .mat format\n",
    "    mat_name='./mat_data/{}'.format(result_file.split('/')[-2])\n",
    "    make_dir(mat_name)\n",
    "    for k in range(len(res_all)):\n",
    "        temp=res_all[k]\n",
    "        mn=temp['model']\n",
    "        dn=temp['data_name']\n",
    "        save_name='{}/{}_{}'.format(mat_name,dn,mn)\n",
    "        sio.savemat(save_name,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_not_flat(model):\n",
    "    # by using not flat inputs, this function performs training, prediction, and evaluation\n",
    "    # for RNN,GRU,LSTM\n",
    "    \n",
    "    #train\n",
    "    model.fit(X_train, y_train)\n",
    "    #prediction\n",
    "    y_test_predicted=model.predict(X_test)\n",
    "    y_test_predicted=y_test_predicted * y_train_std + y_train_mean\n",
    "    #evaluation\n",
    "    corr,R2,mae,y_test_predicted_smooth,corr_smooth,R2_smooth,mae_smooth=calc(y_test,y_test_predicted)\n",
    "    #plot\n",
    "    if plo==1:\n",
    "        plot_all(y_test,y_test_predicted,out,data_name,model_name,target_name)\n",
    "        \n",
    "    res={'data_name':data_name,\n",
    "         'y_test':y_test,\n",
    "         'y_test_predicted':y_test_predicted,\n",
    "         'corr':corr,\n",
    "         'R2':R2,\n",
    "         'mae':mae,\n",
    "         'y_test_predicted_smooth':y_test_predicted_smooth,\n",
    "         'corr_smooth':corr_smooth,\n",
    "         'R2_smooth':R2_smooth,\n",
    "         'mae_smooth':mae_smooth,\n",
    "         'model':model_name,\n",
    "         'n_neuron':n_neuron}\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity2(position):\n",
    "    lenposi=len(position)\n",
    "    v_cm_s=[]\n",
    "    actual_posi=[]\n",
    "    v_m=[];\n",
    "    actual_posi=position*(40.0/200.0)\n",
    "    for k in range(lenposi):\n",
    "        tmp=0\n",
    "        \n",
    "        for i in range(-4,3): #313 filter\n",
    "            if (k+i <= 0 or k+i+1 >= lenposi):\n",
    "                tmp=tmp\n",
    "            else:\n",
    "                xaya = actual_posi[k+i+1] - actual_posi[k+i]             \n",
    "                dist = np.sqrt(np.sum(xaya**2))\n",
    "                tmp=tmp+dist\n",
    "        v_cm_s.append(tmp*(3.0/7.0)) #313 filter\n",
    "    v_m=np.array(v_cm_s)\n",
    "    \n",
    "    return  v_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head3(position):#tangent value\n",
    "    lenposi=len(position)\n",
    "    actual_posi=[]\n",
    "    head_posi_s=[]\n",
    "    head_posi_s2=[]\n",
    "    head_posi_s3=[]\n",
    "    head_posi=[]\n",
    "    actual_posi=position*(40.0/200.0)\n",
    "    for k in range(lenposi):\n",
    "        if(k <=2 or k >= lenposi-4):\n",
    "            tmp=0\n",
    "        else:\n",
    "            xaya=actual_posi[k+3]-actual_posi[k-3]\n",
    "            xa=xaya[0]\n",
    "            ya=xaya[1]        \n",
    "            tmp=math.atan2(ya,xa)\n",
    "        tmp=tmp+math.pi/2.0\n",
    "        head_posi_s.append(tmp)\n",
    "    head_posi_s2=np.array(head_posi_s)\n",
    "    headlen=len(head_posi_s2)\n",
    "    for k in range(headlen):\n",
    "        if (head_posi_s[k] > math.pi):\n",
    "            tmp2=head_posi_s[k]-2*math.pi\n",
    "        else:\n",
    "            tmp2=head_posi_s[k]\n",
    "        head_posi_s3.append(tmp2)\n",
    "    #from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    head_posi=np.array(head_posi_s3)\n",
    "    #print head_posi\n",
    "    return head_posi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_th2(position,th):#Use this function if you want to run decoding when mice are running.\n",
    "    vm = velocity2(position)\n",
    "    use_time_v=vm>=th\n",
    "    plt.plot(vm)\n",
    "    print(np.sum(use_time_v))\n",
    "    return use_time_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_th3(position,th):#Use this function if you want to run decoding when mice are stopping.\n",
    "    vm = velocity2(position)\n",
    "    use_time_v=vm<th\n",
    "    plt.plot(vm)\n",
    "    print(np.sum(use_time_v))\n",
    "    return use_time_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir : ./result_Pos_neurate_0_vf_1.0\n",
      "./data/alldata/092717 OF SERT WT M32-n1\n",
      "2180\n",
      "79 neurons are used\n",
      "mkdir : ./result_Pos_neurate_0_vf_1.0/figure_lstm\n",
      "lstm : corr : [0.6806690066313473, 0.6880130781231317]\n",
      "----\n",
      "./data/alldata/092217 OF CaMKII HKO M30-n1\n",
      "4054\n",
      "59 neurons are used\n",
      "lstm : corr : [0.21678641785410918, 0.24860366583758525]\n",
      "----\n",
      "./data/alldata/091317 OF CaMKII HKO M20-n1\n",
      "2921\n",
      "80 neurons are used\n",
      "lstm : corr : [-0.13482203644745888, 0.2206340709799211]\n",
      "----\n",
      "./data/alldata/090817 OF CaMKII HKO M22-n1\n",
      "2893\n",
      "69 neurons are used\n",
      "lstm : corr : [0.3449884946203825, 0.4807512298557625]\n",
      "----\n",
      "./data/alldata/092217 OF CaMKII WT M29-n1\n",
      "2269\n",
      "63 neurons are used\n",
      "lstm : corr : [0.6507311423389146, 0.5673733067382011]\n",
      "----\n",
      "./data/alldata/M46_042718_OF\n",
      "3072\n",
      "89 neurons are used\n",
      "lstm : corr : [0.787377231931056, 0.7600616100237094]\n",
      "----\n",
      "./data/alldata/081117 OF B6J M27-n1\n",
      "3379\n",
      "57 neurons are used\n",
      "lstm : corr : [0.8096559065141203, 0.7538623364638083]\n",
      "----\n",
      "./data/alldata/091317 OF CaMKII HKO M19-n1\n",
      "3099\n",
      "53 neurons are used\n",
      "lstm : corr : [0.5275856359974085, 0.47821280800356786]\n",
      "----\n",
      "./data/alldata/CK_WT_RN3_OF\n",
      "3203\n",
      "112 neurons are used\n",
      "lstm : corr : [0.6054808298730124, 0.6364848392936239]\n",
      "----\n",
      "./data/alldata/M45_042718_OF\n",
      "2429\n",
      "65 neurons are used\n",
      "lstm : corr : [0.3544462472144691, 0.4833353633359462]\n",
      "----\n",
      "./data/alldata/CK_KO_RN1_OF\n",
      "3815\n",
      "23 neurons are used\n",
      "lstm : corr : [0.22830485049538704, 0.3550993856146341]\n",
      "----\n",
      "./data/alldata/M44_042718_OF\n",
      "3556\n",
      "33 neurons are used\n",
      "lstm : corr : [0.7471676487512895, 0.6594845602452324]\n",
      "----\n",
      "mkdir : ./mat_data/result_Pos_neurate_0_vf_1.0\n",
      "mkdir : ./result_Pos_neurate_0.5_vf_1.0\n",
      "./data/alldata/092717 OF SERT WT M32-n1\n",
      "2180\n",
      "39 neurons are used\n",
      "mkdir : ./result_Pos_neurate_0.5_vf_1.0/figure_lstm\n",
      "lstm : corr : [0.15845288053681805, 0.4142033812101633]\n",
      "----\n",
      "./data/alldata/092217 OF CaMKII HKO M30-n1\n",
      "4054\n",
      "29 neurons are used\n",
      "lstm : corr : [0.1522702550455608, 0.03102698036507502]\n",
      "----\n",
      "./data/alldata/091317 OF CaMKII HKO M20-n1\n",
      "2921\n",
      "40 neurons are used\n",
      "lstm : corr : [-0.1412675165287565, 0.2261554598137961]\n",
      "----\n",
      "./data/alldata/090817 OF CaMKII HKO M22-n1\n",
      "2893\n",
      "35 neurons are used\n",
      "lstm : corr : [0.21925825165397697, 0.26837112877421265]\n",
      "----\n",
      "./data/alldata/092217 OF CaMKII WT M29-n1\n",
      "2269\n",
      "31 neurons are used\n",
      "lstm : corr : [0.27941900942334813, 0.24252245491003666]\n",
      "----\n",
      "./data/alldata/M46_042718_OF\n",
      "3072\n",
      "45 neurons are used\n",
      "lstm : corr : [0.380017435827882, 0.546048740860836]\n",
      "----\n",
      "./data/alldata/081117 OF B6J M27-n1\n",
      "3379\n",
      "29 neurons are used\n",
      "lstm : corr : [0.5846293057445223, 0.5401923578455067]\n",
      "----\n",
      "./data/alldata/091317 OF CaMKII HKO M19-n1\n",
      "3099\n",
      "27 neurons are used\n",
      "lstm : corr : [0.3067377153852711, 0.11738195335013395]\n",
      "----\n",
      "./data/alldata/CK_WT_RN3_OF\n",
      "3203\n",
      "56 neurons are used\n",
      "lstm : corr : [0.27422640976166623, 0.359530240999616]\n",
      "----\n",
      "./data/alldata/M45_042718_OF\n",
      "2429\n",
      "33 neurons are used\n",
      "lstm : corr : [-0.11143153037310331, 0.29941351016144124]\n",
      "----\n",
      "./data/alldata/CK_KO_RN1_OF\n",
      "3815\n",
      "11 neurons are used\n",
      "lstm : corr : [0.10195976896352718, 0.032341833641244315]\n",
      "----\n",
      "./data/alldata/M44_042718_OF\n",
      "3556\n",
      "17 neurons are used\n",
      "lstm : corr : [0.5335908177483861, 0.39565765373377754]\n",
      "----\n",
      "mkdir : ./mat_data/result_Pos_neurate_0.5_vf_1.0\n"
     ]
    }
   ],
   "source": [
    "velocity_th_list=[1.0]#Velocity Filter cm/sec\n",
    "using_neuron_rate_list = [0, 0.5] #0: all neurons will be used, 0.25: 75% of neurons, 0.5: half, 0.75: 25% of neurons \n",
    "info_list= [0] #Specify the Deletion order #0: actual index, 1: negacon index\n",
    "# to change the deletion order, modify the name of information list in function.py. please see line 139 of function.py.\n",
    "\n",
    "#Note that when you use negacon=2 and neurate, 0:all neurons, 0.25: 25% of neurons, 0.5: half, 0.75:75% of neurons\n",
    "#######################\n",
    "#result output directory\n",
    "\n",
    "\n",
    "for th_velo, info_ind, neurate in product(velocity_th_list, info_list, using_neuron_rate_list):\n",
    "    out='./result_Pos_neurate_{}_vf_{}'.format(neurate, th_velo)\n",
    "    make_dir(out)\n",
    "\n",
    "    result_dir=out#dir of decoding result\n",
    "    result_file='{}/res_all.pkl'.format(result_dir)\n",
    "    \n",
    "    data_dir='./data/alldata//'\n",
    "    \n",
    "    #choose data you want to use for decoding\n",
    "    datalist_all=glob.glob('{}/*'.format(data_dir))\n",
    "    \n",
    "\n",
    "    plo=0 # set 1, if you want to plot the decoding result\n",
    "    negacon=0 # set 1, if you want to shuffle the data and make negative control; set 2, if you want to calculate negacon data for neurate.\n",
    "    zscore=0 # set 1, if you want to standarlize the position for machine learning models\n",
    " \n",
    "    \n",
    "    ###hyper parameters\n",
    "    #for kalman filter\n",
    "    lag=10\n",
    "    c=1\n",
    "    #for PF\n",
    "    sig_PF=10\n",
    "    #for neural networks\n",
    "    epoch=10\n",
    "    unit=1000\n",
    "    dropout=0.1\n",
    "    \n",
    "    \n",
    "    #######################\n",
    "    np.random.seed(100)\n",
    "\n",
    "    res_all=[]\n",
    "    pf_score_all=[]\n",
    "    for datalist_name in datalist_all:\n",
    "        print(datalist_name)\n",
    "        datalist=glob.glob(datalist_name + '/*')\n",
    "        data_name=datalist[0].split('/')[-2]\n",
    "\n",
    "        #load data\n",
    "        info, trace,spike,position,x_position,y_position,n_neuron,n_time,firing_rate,trace_smooth=data_load3(datalist)\n",
    "        \n",
    "        if 'cam30' in data_dir.split('/')[2]:# if data used is camkii hko data of 60 minutes, I use first 30 minutes\n",
    "            trace=trace[:5395,:]\n",
    "            spike=spike[:5395,:]\n",
    "            position=position[:5395,:]\n",
    "            x_position=x_position[:5395]\n",
    "            y_position=y_position[:5395]\n",
    "            n_time=5395\n",
    "            firing_rate=firing_rate[:5395,:]\n",
    "            trace_smooth=trace_smooth[:5395,:]\n",
    "\n",
    "        #thresholding by velocity\n",
    "        use_time_v=v_th2(position,th_velo)\n",
    "       \n",
    "        trace=trace[use_time_v,:]\n",
    "        spike=spike[use_time_v,:]\n",
    "        position=position[use_time_v,:]\n",
    "        x_position=x_position[use_time_v]\n",
    "        y_position=y_position[use_time_v]\n",
    "        n_time=np.sum(use_time_v)\n",
    "        firing_rate=firing_rate[use_time_v,:]\n",
    "        trace_smooth=trace_smooth[use_time_v,:]\n",
    "        \n",
    "        # I dont use these options, but original code contain these options and I leave it\n",
    "        bins_before=0 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        ## Set what part of data should be part of the training/testing sets\n",
    "        train_range=[0, 0.5]\n",
    "        test_range=[0.5, 1.0]\n",
    "\n",
    "        train_set=np.arange(np.int(np.round(train_range[0]*n_time))+bins_before,np.int(np.round(train_range[1]*n_time))-bins_after)\n",
    "        test_set=np.arange(np.int(np.round(test_range[0]*n_time))+bins_before,np.int(np.round(test_range[1]*n_time))-bins_after)\n",
    "            \n",
    "        use_data=np.sum(spike[train_set,:],axis=0)>0#remove the neuron which dont activate at training\n",
    "        spike=spike[:,use_data]\n",
    "        trace=trace[:,use_data]\n",
    "        firing_rate=firing_rate[:,use_data]\n",
    "        trace_smooth=trace_smooth[:,use_data]\n",
    "        info=info[use_data]\n",
    "\n",
    "        n_neuron=spike.shape[1]\n",
    "            \n",
    "        firing_rate_for_score=firing_rate\n",
    "        if negacon==1:# in the case of negacon, I randomly reorder spike sequence for training\n",
    "            for k in range(n_neuron):\n",
    "                r=np.random.permutation(len(train_set))\n",
    "                spike[train_set,k]=spike[train_set,k][r]\n",
    "            firing_rate=estimate_firing_rate(spike)\n",
    "\n",
    "            for k in range(n_neuron):\n",
    "                r=np.random.permutation(n_time)\n",
    "                spike[:,k]=spike[r,k]\n",
    "            firing_rate_for_score=estimate_firing_rate(spike)\n",
    "\n",
    "\n",
    "        #####\n",
    "        neural_data=firing_rate # neural activity data used in decoding\n",
    "    \n",
    "        if negacon == 2:\n",
    "            [neural_data,n_neuron] = neuron_thresholding3(neural_data,position,train_set,neurate)\n",
    "        else:\n",
    "            [neural_data,n_neuron,info_ind] = neuron_thresholding2(neural_data,position,train_set,neurate,info,info_ind)\n",
    "            \n",
    "        neural_data=neural_data-np.mean(neural_data,axis=0,keepdims=True)\n",
    "        neural_data_for_score=firing_rate_for_score # neural activity data used in calculating correlation between neural activity and position\n",
    "        neural_data_for_score=neural_data_for_score-np.mean(neural_data_for_score,axis=0,keepdims=True)\n",
    "        #####\n",
    "\n",
    "        X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "        X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "        ######################## choose the types of decoding, position, speed, or motion direction.\n",
    "        target_name = ['x_position','y_position'] #target variable name for plot\n",
    "        target_is_position = True\n",
    "        y=position[:] #target variable. shape is (n_time , 2)\n",
    "        \n",
    "        #target_name = ['head direction'] #target variable name for plot\n",
    "        #y=head3(position).reshape(-1,1) #target variable. shape is (n_time , 1)\n",
    "        #target_is_position = False\n",
    "        \n",
    "        #target_name = ['speed'] #target variable name for plot\n",
    "        #y=velocity2(position).reshape(-1,1) #target variable. shape is (n_time , 1)\n",
    "        #target_is_position = False\n",
    "        ########################\n",
    "\n",
    "        ### 3C. Split into training / testing / testation sets\n",
    "\n",
    "        #Get training data\n",
    "        X_train=X[train_set,:,:]\n",
    "        X_flat_train=X_flat[train_set,:]\n",
    "        y_train=y[train_set,:]\n",
    "\n",
    "        #Get testing data\n",
    "        X_test=X[test_set,:,:]\n",
    "        X_flat_test=X_flat[test_set,:]\n",
    "        y_test=y[test_set,:]\n",
    "\n",
    "        #Z-score \"X\" inputs. \n",
    "        X_train_mean=np.nanmean(X_train,axis=0)\n",
    "        X_train_std=np.nanstd(X_train,axis=0)\n",
    "        X_train=(X_train-X_train_mean)/X_train_std\n",
    "        X_test=(X_test-X_train_mean)/X_train_std\n",
    "\n",
    "        #Z-score \"X_flat\" inputs. \n",
    "        X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "        X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "        X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "        X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "\n",
    "        #Zero-center or zscore outputs\n",
    "        y_train_mean=np.mean(y_train,axis=0)\n",
    "        y_train_std=np.std(y_train,axis=0)\n",
    "        y_train_std_svr=np.std(y_train,axis=0)\n",
    "        if zscore==0:\n",
    "            y_train_std=y_train_std*0 +1\n",
    "        y_train_svr=(y_train-y_train_mean)/y_train_std_svr#data for svr is standarized if zscore =0\n",
    "        y_train=(y_train-y_train_mean)/y_train_std\n",
    "\n",
    "        ## 4. Run Decoders-----------------------------------------------------------------------\n",
    "         \n",
    "\n",
    "        ### 4H. LSTM (Long Short Term Memory)\n",
    "        model_name='lstm'\n",
    "        model=LSTMDecoder(units=unit,dropout=dropout,num_epochs=epoch)\n",
    "        res=model_eval_not_flat(model)\n",
    "        res_all.append(res)\n",
    "        print('{} : corr : {}'.format(model_name,res['corr_smooth']))\n",
    " \n",
    "\n",
    "        plt.close('all')\n",
    "        print('----')\n",
    "\n",
    "    #ensemble\n",
    "    df=pd.DataFrame(res_all)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    with open('{}/res_all.pkl'.format(out),'wb') as f:\n",
    "        pickle.dump(res_all,f)\n",
    "\n",
    "    result={#'res_all':res_all,\n",
    "            'bins_after':bins_after,\n",
    "            'bins_current':bins_current,\n",
    "            'bins_before':bins_before,\n",
    "            'epoch':epoch,\n",
    "            'dropout':dropout,\n",
    "            'unit':unit,\n",
    "            'negacon':negacon,\n",
    "            'C':c,\n",
    "            'lag':lag,\n",
    "            'sig_PF':sig_PF,\n",
    "            'zscore':zscore,\n",
    "            'PF_score':pf_score_all,\n",
    "            'th_velo':th_velo,\n",
    "            'neurate':neurate}\n",
    "\n",
    "    with open('{}/parameters.pkl'.format(out),'wb') as f:\n",
    "        pickle.dump(result,f)\n",
    "        \n",
    "    ##Generate mat file\n",
    "    make_mat_file(res_all,result_file)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
